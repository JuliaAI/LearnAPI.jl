var documenterSearchIndex = {"docs":
[{"location":"patterns/missing_value_imputation/#Missing-Value-Imputation","page":"Missing Value Imputation","title":"Missing Value Imputation","text":"","category":"section"},{"location":"patterns/survival_analysis/#Survival-Analysis","page":"Survival Analysis","title":"Survival Analysis","text":"","category":"section"},{"location":"model_traits/#Model-Traits","page":"Model Traits","title":"Model Traits","text":"","category":"section"},{"location":"model_traits/","page":"Model Traits","title":"Model Traits","text":"trait fallback value return value example\nLearnAPI.ismodel(model) false is true for any model, as defined in Models true\nLearnAPI.implemented_methods(model) () lists of all overloaded/implemented methods (traits excluded) (:fit, :predict)\nLearnAPI.target_proxy_kind(model) () details form of target proxy output (predict= LearnAPI.Distribution,)\nLearnAPI.position_of_target(model) 0 † the positional index of the target in data in fit(..., data...; metadata) calls 2\nLearnAPI.position_of_weights(model) 0 † the positional index of observation weights in data in fit(..., data...; metadata) 3\nLearnAPI.descriptors(model) () lists one or more suggestive model descriptors from LearnAPI.descriptors() (:classifier, :probabilistic)\nLearnAPI.is_pure_julia(model) false is true if implementation is 100% Julia code true\nLearnAPI.pkg_name(model) \"unknown\" name of package providing core algorithm (may be different from package providing LearnAPI.jl implementation) \"DecisionTree\"\nLearnAPI.doc_url`(model) \"unknown\" url providing documentation of the core algorithm \"https://en.wikipedia.org/wiki/Decisiontreelearning\"\nLearnAPI.pkg_license(model) \"unknown\" name of license of package providing core algorithm \"MIT\"\nLearnAPI.load_path(model) \"unknown\" a string indicating where the struct typeof(model) is defined, beginning with name of package providing implementation FastTrees.LearnAPI.DecisionTreeClassifier\nLearnAPI.is_wrapper(model) false is true if one or more hyperparameters are themselves models true\nLearnAPI.fit_keywords(model) () tuple of symbols for keyword arguments accepted by fit (metadata) (:class_weights,)\nLearnAPI.human_name(model) type name with spaces, acronymns preserved human name for the model; should be a noun \"elastic net regressor\"\nLearnAPI.iteration_parameter(model) nothing symbolic name of an iteration parameter :epochs","category":"page"},{"location":"model_traits/","page":"Model Traits","title":"Model Traits","text":"† If the value is 0, then the variable in boldface type is not supported and not expected to appear in data. If length(data) is less than the trait value, then data is understood to exclude the variable, but note that fit can have multiple signatures of varying lengths, as in fit(model, verbosity, X, y) and fit(model, verbosity, X, y, w). A non-zero value is a promise that fit includes a signature of sufficient length to include the variable.","category":"page"},{"location":"patterns/classifiers/#Classifiers","page":"Classifiers","title":"Classifiers","text":"","category":"section"},{"location":"common_implementation_patterns/#Common-Implementation-Patterns","page":"Common Implementation Patterns","title":"Common Implementation Patterns","text":"","category":"section"},{"location":"common_implementation_patterns/","page":"Common Implementation Patterns","title":"Common Implementation Patterns","text":"warning: Warning\nThis section is only an implementation guide. The definitive specification of the Learn API is given in Reference.","category":"page"},{"location":"common_implementation_patterns/","page":"Common Implementation Patterns","title":"Common Implementation Patterns","text":"This guide is intended to be consulted after reading Anatomy of a Model Implementation, which introduces the main interface objects and terminology.","category":"page"},{"location":"common_implementation_patterns/","page":"Common Implementation Patterns","title":"Common Implementation Patterns","text":"Although an implementation is defined purely by the methods and traits it implements, most implementations fall into one (or more) of the following informally understood patterns or \"tasks\":","category":"page"},{"location":"common_implementation_patterns/","page":"Common Implementation Patterns","title":"Common Implementation Patterns","text":"Classifiers: Supervised learners for categorical targets\nRegressors: Supervised learners for continuous targets\nIterative Models\nIncremental Models\nStatic Transformers: Transformations that do not learn but which have hyper-parameters and/or deliver ancilliary information about the transformation\nDimension Reduction: Transformers that learn to reduce feature space dimension\nMissing Value Imputation: Transformers that replace missing values.\nClusterering: Algorithms that group data into clusters for classification and possibly dimension reduction. May be true learners (generalize to new data) or static.\nOutlier Detection: Supervised, unsupervised, or semi-supervised learners for anomaly detection.\nLearning a Probability Distribution: Models that fit a distribution or distribution-like object to data\nTime Series Forecasting\nTime Series Classification\nSupervised Bayesian Models\nSurvival Analysis","category":"page"},{"location":"patterns/static_transformers/#Static-Transformers","page":"Static Transformers","title":"Static Transformers","text":"","category":"section"},{"location":"patterns/supervised_bayesian_models/#Supervised-Bayesian-Models","page":"Supervised Bayesian Models","title":"Supervised Bayesian Models","text":"","category":"section"},{"location":"patterns/regressors/#Regressors","page":"Regressors","title":"Regressors","text":"","category":"section"},{"location":"patterns/time_series_classification/#Time-Series-Classifiction","page":"Time Series Classifiction","title":"Time Series Classifiction","text":"","category":"section"},{"location":"anatomy_of_an_implementation/#Anatomy-of-an-Implementation","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Summary. A model is just a container for hyper-parameters. A basic implementation of the ridge regressor requires implementing fit and predict methods dispatched on the model type; predict is an example of an operation (another is transform). In this example we also implement an accessor function called feature_importance (returning the absolute values of the linear coefficients). The ridge regressor has a target variable and one trait declaration flags the output of predict as being a proxy for the target. Other traits articulate the model's training data type requirements and the output type of predict.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"We begin by describing an implementation of LearnAPI.jl for basic ridge regression (no intercept) to introduce the main actors in any implementation.","category":"page"},{"location":"anatomy_of_an_implementation/#Defining-a-model-type","page":"Anatomy of an Implementation","title":"Defining a model type","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"The first line below imports the lightweight package LearnAPI.jl whose methods we will be extending, the second, libraries needed for the core algorithm.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"using LearnAPI\nusing LinearAlgebra, Tables","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Next, we define a struct to store the single hyper-parameter lambda of this model:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"struct MyRidge <: LearnAPI.Model\n        lambda::Float64\nend","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"The subtyping MyRidge <: LearnAPI.Model is optional but recommended where it is not otherwise disruptive. If you omit the subtyping then you must declare","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"LearnAPI.ismodel(::MyRidge) = true","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"as a promise that instances of MyRidge implement LearnAPI.jl.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Instances of MyRidge are called models and MyRidge is a model type.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"A keyword argument constructor providing defaults for all hyper-parameters should be provided:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"MyRidge(; lambda=0.1) = MyRidge(lambda)","category":"page"},{"location":"anatomy_of_an_implementation/#A-method-to-fit-the-model","page":"Anatomy of an Implementation","title":"A method to fit the model","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"A ridge regressor requires two types of data for training: input features X and a target y. Training is implemented by overloading fit. Here verbosity is an integer (0 should train silently, unless warnings are needed):","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"function LearnAPI.fit(model::MyRidge, verbosity, X, y)\n\n        # process input:\n        x = Tables.matrix(X)  # convert table to matrix\n        features = Tables.columnnames(X)\n\n        # core solver:\n        coefficients = (x'x + model.lambda*I)\\(x'y)\n\n        # prepare output - learned parameters:\n        fitted_params = (; coefficients)\n\n        # prepare output - model state:\n        state = nothing  # not relevant here\n\n        # prepare output - byproducts of training:\n        feature_importances =\n                [features[j] => abs(coefficients[j]) for j in eachindex(features)]\n        sort!(feature_importances, by=last) |> reverse!\n        verbosity > 1 && @info \"Features in order of importance: $(first.(feature_importances))\"\n        report = (; feature_importances)\n\n        return fitted_params, state, report\nend","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Regarding the return value of fit:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"The fitted_params variable is for the model's learned parameters, for passing to predict (see below).\nThe state variable is only relevant when additionally implementing a LearnAPI.update! or LearnAPI.ingest! method (see Fit, update! and ingest!).\nThe report is for other byproducts of training, excluding the learned parameters.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Notice that we have chosen here to suppose that X is presented as a table (rows are the observations); and we suppose y is a Real vector. This is not a restriction on types placed by LearnAPI.jl. However, we can articulate our model's particular type requirements with the LearnAPI.fit_data_scitype trait; see Training data types below.","category":"page"},{"location":"anatomy_of_an_implementation/#Operations","page":"Anatomy of an Implementation","title":"Operations","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Now we need a method for predicting the target on new input features:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"function LearnAPI.predict(::MyRidge, fitted_params, Xnew)\n    Xmatrix = Tables.matrix(Xnew)\n    report = nothing\n    return Xmatrix*fitted_params.coefficients, report\nend","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"In some models predict computes something of interest in addition to the target prediction, and this report item is returned as the second component of the return value. When there's nothing to report, we must return nothing, as here.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Our predict method is an example of an operation. Other operations include transform and inverse_transform and a model can implement more than one. For example, a K-means clustering model might implement a transform for dimension reduction, and a predict to return cluster labels.","category":"page"},{"location":"anatomy_of_an_implementation/#Accessor-functions","page":"Anatomy of an Implementation","title":"Accessor functions","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"The arguments of an operation are always (model, fitted_params, data...). The interface also provides accessor functions for extracting information, from the fitted_params and/or report, that is shared by several model types.  There is one for feature importances that we can implement for MyRidge:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"LearnAPI.feature_importances(::MyRidge, fitted_params, report) =\nreport.feature_importances","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Another example of an accessor function is training_losses.","category":"page"},{"location":"anatomy_of_an_implementation/#traits","page":"Anatomy of an Implementation","title":"Model traits","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Our model has a target variable, in the sense outlined in Scope and undefined notions, and predict returns an object with exactly the same form as the target. We indicate this behaviour by declaring","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"LearnAPI.target_proxy_kind(::Type{<:MyRidge}) = (; predict=LearnAPI.TrueTarget())","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Or, you can use the shorthand","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait MyRidge target_proxy_kind = (; predict=LearnAPI.TrueTarget())","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"More generally, predict only returns a proxy for the target, such as probability distributions, and we would make a different declaration here. See Target proxies for details.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"LearnAPI.target_proxy_kind is an example of a model trait. A complete list of traits and the contracts they imply is given in Model Traits.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"MLJ only. The values of all traits constitute a model's metadata, which is recorded in the searchable MLJ Model Registry, assuming the implementation-providing package is registered there.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"We also need to indicate that the target appears in training (this is a supervised model). We do this by declaring where in the list of training data arguments (in this case (X, y)) the target variable (in this case y) appears:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait MyRidge position_of_target = 2","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"As explained in the introduction, LearnAPI.jl does not attempt to define strict model \"types\", such as \"regressor\" or \"clusterer\". However, we can optionally specify suggestive descriptors, as in","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait MyRidge descriptors = (:regression,)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"but note that this declaration promises nothing. Do LearnAPI.descriptors() to get a list of available descriptors.","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Finally, we are required to declare what methods (excluding traits) we have explicitly overloaded for our type:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait MyRidge implemented_methods = (\n        :fit,\n        :predict,\n        :feature_importances,\n)","category":"page"},{"location":"anatomy_of_an_implementation/#Training-data-types","page":"Anatomy of an Implementation","title":"Training data types","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Optional trait declarations articulate the permitted types for training data. To be precise, an implementation makes scientific type declarations, which in this case look like:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"using ScientificTypesBase\n@trait MyRidge fit_data_scitype = Tuple{Table(Continuous), AbstractVector{Continuous}}","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"This is a contract that data is acceptable in the call fit(model, verbosity, data...) whenever","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"scitype(data) <: Tuple{Table(Continuous), AbstractVector{Continuous}}","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Or, in other words:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"X in fit(model, verbosity, X, y) is acceptable, provided scitype(X) <: Table(Continuous) - meaning that X is a Tables.jl compatible table whose columns have some <:AbstractFloat element type.\ny in fit(model, verbosity, X, y) is acceptable if scitype(y) <: AbstractVector{Continuous} - meaning that it is an abstract vector with <:AbstractFloat elements.","category":"page"},{"location":"anatomy_of_an_implementation/#Types-for-data-returned-by-operations","page":"Anatomy of an Implementation","title":"Types for data returned by operations","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"A promise that an operation, such as predict, returns an object of given scientific type is articulated in this way:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait return_scitypes = (:predict => AbstractVector{<:Continuous},)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"If predict had instead returned probability distributions, and these implement the Distributions.pdf interface, then the declaration would be","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@trait return_scitypes = (:predict => AbstractVector{Density{<:Continuous}},)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"There is also an input_scitypes trait for operations. However, this falls back to the scitype for the first argument of fit, as inferred from fit_data_scitype (see above). So we need not overload it here.","category":"page"},{"location":"anatomy_of_an_implementation/#workflow","page":"Anatomy of an Implementation","title":"Illustrative fit/predict workflow","text":"","category":"section"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Here's some toy data for supervised learning:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"using Tables\n\nn = 10          # number of training observations\ntrain = 1:6\ntest = 7:10\n\na, b, c = rand(n), rand(n), rand(n)\nX = (; a, b, c) |> Tables.rowtable\ny = 2a - b + 3c + 0.05*rand(n)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Instantiate a model with relevant hyperparameters:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"model = MyRidge(lambda=0.5)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Train the model:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"import LearnAPI: fit, predict, feature_importances\n\nfitted_params, state, fit_report = fit(model, 1, X[train], y[train])","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Inspect the learned paramters and report:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"@info \"training outcomes\" fitted_params report","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Inspect feature importances:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"feature_importances(model, fitted_params, report)","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Make a prediction using new data:","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"yhat, predict_report = predict(model, fitted_params, X[test])","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"Compare predictions with ground truth","category":"page"},{"location":"anatomy_of_an_implementation/","page":"Anatomy of an Implementation","title":"Anatomy of an Implementation","text":"deviations = yhat - y[test]\nloss = deviations .^2 |> sum\n@info \"Sum of squares loss\" loss","category":"page"},{"location":"patterns/clusterering/#Clusterering","page":"Clusterering","title":"Clusterering","text":"","category":"section"},{"location":"patterns/iterative_models/#Iterative-Models","page":"Iterative Models","title":"Iterative Models","text":"","category":"section"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Here we give the definitive specification of interface provided by LearnAPI.jl. For a more informal guide see Common Implementation Patterns.","category":"page"},{"location":"reference/#Models","page":"Reference","title":"Models","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Summary In LearnAPI.jl a model is a Julia object whose properties are the hyper-parameters of some learning algorithm. Functionality is created by overloading methods defined by the interface and promises of certain behavior are articulated by model traits.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"In this document the word \"model\" has a very specific meaning that may differ from the reader's common understanding of the word - in statistics, for example. In this document a model is any julia object, some_model say, storing the hyper-parameters of some learning algorithm that are accessible as named properties of the model, as in some_model.epochs. Calling Base.propertynames(some_model) must return the names of those hyper-parameters.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"It is supposed that making copies of model objects is a cheap operation. Consequently, learned parameters, such as weights in a neural network (the fitted_params described in Fit, update! and ingest!) are not expected to be part of a model. Storing learned parameters in a model is not explicitly ruled out, but doing so might lead to performance issues in packages adopting LearnAPI.jl.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Two models with the same type should be == if and only if all their hyper-parameters are ==. Of course, a hyper-parameter could be another model.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Any instance of SomeType below is a model in the above sense:","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"struct SomeType{T<:Real} <: LearnAPI.Model\n    epochs::Int\n    lambda::T\nend","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"The subtyping <: LearnAPI.Model is optional. If it is included and the type is instead a mutable struct, then there is no need to explicitly overload Base.==. If it is omitted, then one must make the declaration","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"LearnAPI.ismodel(::SomeType) = true","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"and overload Base.== if the type is mutable.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"A keyword constructor providing default values for all non-model hyper-parameters is required. If a model has other models as hyper-parameters, its LearnAPI.is_wrapper trait must be set to true.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"MLJ only. The subtyping also ensures instances will be displayed according to a standard MLJ convention, assuming MLJ or MLJBase is loaded.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"LearnAPI.ismodel\nLearnAPI.Model","category":"page"},{"location":"reference/#LearnAPI.ismodel","page":"Reference","title":"LearnAPI.ismodel","text":"ismodel(m)\n\nReturns true exactly when m is a model, as defined in the LearnAPI.jl documentation. In particular, this means:\n\nm is an object whose properties, as returned by getproperty(m, field) for field in propertynames(m), represent the hyper-parameters of a machine learning algorithm.\nIf n is another model, then m == n if and only if typeof(n) == typeof(m) and corresponding properties are ==.\nm correctly implements zero or more methods from LearnAPI.jl. See the LearnAPI.jl documentation for details.\n\nNew model implementations\n\nEither declare NewModelType <: LearnAPI.Model or LearnAPI.model(::NewModelType) = true.\n\nSee also LearnAPI.Model.\n\n\n\n\n\n","category":"function"},{"location":"reference/#LearnAPI.Model","page":"Reference","title":"LearnAPI.Model","text":"LearnAPI.Model\n\nAn optional abstract type for models implementing LearnAPI.jl.\n\nNew model implementations\n\nEither declare NewModelType <: LearnAPI.Model or LearnAPI.model(::SomeModelType) = true. The first implies the second and additionally guarantees == has correct behaviour for NewModelType instances.\n\nSee also LearnAPI.ismodel.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Methods","page":"Reference","title":"Methods","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"None of the methods described in the linked sections below are compulsory, but any implemented or overloaded method that is not a model trait must be added to the return value of LearnAPI.implemented_methods, as in","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"LearnAPI.implemented_methods(::Type{<SomeModelType}) = (:fit, update!, predict)","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"For examples, see Anatomy of an Interface.","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Fit, update! and ingest!: for models that \"learn\" (generalize to new data)\nOperations: predict, transform and their relatives\nAccessor Functions: accessing certain byproducts of training that many models share, such as feature importances and training losses\nModel Traits: contracts for specific behaviour, such as \"The second data argument of fit is a target variable\" or \"I predict probability distributions\".","category":"page"},{"location":"operations/#operations","page":"Predict and other operations","title":"Predict and Other Operations","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"Summary Methods like predict and transform, that generally depend on learned parameters, are called operations. All implemented operations must be included in the output of the implemented_methods model trait. When an operation returns a target proxy, it must make a target_proxy_kind declaration.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"An operation is any method with signature some_operation(model, fitted_params, data...). Here fitted_params is the learned parameters object, as returned by LearnAPI.fit(model, ...), which will be nothing if fit is not implemented (true for models that do not generalize to new data). For example, LearnAPI.predict in the following code snippet is an operation:","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"fitted_params, state, fit_report = LearnAPI.fit(some_model, 1, X, y)\nŷ, predict_report = LearnAPI.predict(some_model, fitted_params, Xnew)","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"method compulsory? fallback requires\nLearnAPI.predict no none \nLearnAPI.predict_mode no none predict\nLearnAPI.predict_mean no none predict\nLearnAPI.predict_median no none predict\nLearnAPI.predict_joint no none \nLearnAPI.transform no none \nLearnAPI.inverse_transform no none transform","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"† MLJ only. MLJBase provides fallbacks for predict_mode, predict_mean and predict_median by broadcasting methods from Statistics and StatsBase over the results of predict.","category":"page"},{"location":"operations/#General-requirements","page":"Predict and other operations","title":"General requirements","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"Only implement predict_joint for outputing a single multivariate probability distribution for multiple target predictions, as described further at LearnAPI.predict_joint.\nEach operation explicitly implemented or overloaded must be included in the return value of LearnAPI.implemented_methods.","category":"page"},{"location":"operations/#Predict-or-transform?","page":"Predict and other operations","title":"Predict or transform?","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"If the model has a target, as defined under Scope and undefined notions, then only predict or predict_joint can be used to generate a corresponding target proxy.\nIf an operation is to have an inverse operation, then it cannot be predict - use transform and inverse_transform.\nIf only a single operation is implemented, and there is no target variable, use transform. ","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"Here an \"inverse\" of transform is very broadly understood as any operation that can be applied to the output of transform to obtain an object of the same form as the input of transform; for example this includes one-sided inverses, and approximate one-sided inverses. ","category":"page"},{"location":"operations/#Target-proxies","page":"Predict and other operations","title":"Target proxies","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"In the case that a model has the concept of a target variable, as described under Scope and undefined notions, the output of predict or predict_joint may have the form of a proxy for the target, such as a vector of truth-probabilities for binary targets.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"We assume the reader is already familiar with the notion of a target variable in supervised learning, but target variables are not limited to supervised models. For example, we may regard the \"outlier\"/\"inlier\" assignments in unsupervised anomaly detection as a target. A target proxy in this example would be probabilities for outlierness, as these can be paired with \"outlier\"/\"inlier\" labels assigned by humans, using, say, area under the ROC curve, to quantify performance.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"Similarly, the integer labels assigned to some observations by a clustering algorithm can be regarded as a target variable. The labels obtained can be paired with human labels using, say, the Rand index. ","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"The kind of proxy one has is informally classified by a subtype of LearnAPI.TargetProxy. These types are intended for dispatch outside of LearnAPI.jl and have no fields.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"type form of an observation\nLearnAPI.TrueTarget same as target observations (possible requirement: observations have same type as target observations)\nLearnAPI.Sampleable object that can be sampled to obtain object of the same form as target observation (possible requirement: observation implements Base.rand)\nLearnAPI.Distribution explicit probability density/mass function whose sample space is all possible target observations (possible requirement: observation implements Distributions.pdf and Base.rand)\nLearnAPI.LogDistribution explicit log probability density/mass function whose sample space is possible target observations (possible requirement: observation implements Distributions.logpdf and Base.rand)\n† LearnAPI.Probability raw numerical probability or probability vector\n† LearnAPI.LogProbability log probability or log probability vector\n† LearnAPI.Parametric a list of parameters (e.g., mean and variance) describing some distribution\nLearnAPI.LabelAmbiguous same form as the (multi-class) target, but selected from new unmatched labels of possibly unequal number (as in, e.g., clustering)\nLearnAPI.LabelAmbiguousSampleable sampleable version of LabelAmbiguous; see Sampleable above\nLearnAPI.LabelAmbiguousDistribution pdf/pmf version of LabelAmbiguous; see Distribution  above\nLearnAPI.ConfidenceInterval confidence interval (possible requirement:  observation isa Tuple{Real,Real})\nLearnAPI.Set finite but possibly varying number of target observations (possible requirement: observation isa Set{target_observation_type})\nLearnAPI.ProbabilisticSet as for Set but labelled with probabilities (not necessarily summing to one)\nLearnAPI.SurvivalFunction survival function (possible requirement: observation is single-argument function mapping Real to Real)\nLearnAPI.SurvivalDistribution probability distribution for survival time (possible requirement: observation have type Distributions.ContinuousUnivariateDistribution)","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"† MLJ only. To avoid ambiguities in representation, these options are disallowed, in favour of preceding alternatives.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"warning: Warning\nThe \"possible requirement\"s listed are not part of LearnAPI.jl.","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"An operation with target proxy as output must declare the TargetProxy subtype using the LearnAPI.target_proxy_kind, as in","category":"page"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"LearnAPI.target_proxy_kind(::Type{<:SomeModel}) = (predict=LearnAPI.Distribution,)","category":"page"},{"location":"operations/#Special-case-of-predict_joint","page":"Predict and other operations","title":"Special case of predict_joint","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"If predict_joint is implemented, then a target_proxy_kind declaration is required, but the interpretation is slightly different. This is because the output of predict_joint is not a number of observations but a single object. See more at LearnAPI.predict_joint below.","category":"page"},{"location":"operations/#Operation-specific-details","page":"Predict and other operations","title":"Operation-specific details","text":"","category":"section"},{"location":"operations/","page":"Predict and other operations","title":"Predict and other operations","text":"LearnAPI.predict\nLearnAPI.predict_mean\nLearnAPI.predict_median\nLearnAPI.predict_joint\nLearnAPI.transform\nLearnAPI.inverse_transform","category":"page"},{"location":"operations/#LearnAPI.predict","page":"Predict and other operations","title":"LearnAPI.predict","text":"LearnAPI.predict(model, fitted_params, data...)\n\nReturn (ŷ, report) where ŷ are the predictions, or prediction-like output (such as probabilities), for a machine learning model model, with learned parameters fitted_params, as returned by a preceding call to LearnAPI.fit(model, ...). Here report contains ancilliary byproducts of the computation, or is nothing; data is a tuple of data objects, generally a single object representing new observations not seen in training. \n\nNew model implementations\n\nIf implemented, include :predict in the tuple returned by the LearnAPI.implemented_methods trait. \n\nIf performance_measureable = true, then ŷ must be:\n\neither an array or table with the same number of observations as each element of data; it cannot be a lazy object, such as a DataLoader\ntarget-like; see  LearnAPI.paradigm for specifics.\n\nOtherwise there are no restrictions on what predict may return, apart from what the implementation itself promises, by making an optional LearnAPI.output_scitypes declaration.\n\nIf predict is computing a target proxy, as defined in the MLJLearn documentation, then a LearnAPI.target_proxy_kind declaration is required, as in\n\nLearnAPI.target_proxy_kind(::Type{<:SomeModel}) = (predict=LearnAPI.Distribution,)\n\nDo LearnAPI.target_proxy_kind() to list the available kinds.\n\nBy default, it is expected that data has length one. Otherwise, LearnAPI.input_scitypes must be overloaded.\n\nSee also LearnAPI.fit, LearnAPI.predict_mean, LearnAPI.predict_mode, LearnAPI.predict_median.\n\n\n\n\n\n","category":"function"},{"location":"operations/#LearnAPI.predict_mean","page":"Predict and other operations","title":"LearnAPI.predict_mean","text":"LearnAPI.predict_mean(model, fitted_params, data...)\n\nSame as LearnAPI.predict except replaces probababilistic predictions with mean values.\n\nNew model implementations\n\nA fallback broadcasts mean over the first return value ŷ of LearnAPI.predict. An algorithm that computes probabilistic predictions may already need to predict mean values, and so overloading this method might enable a performance boost.\n\nIf overloaded, include :predict_mean in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.predict, LearnAPI.fit.\n\n\n\n\n\n","category":"function"},{"location":"operations/#LearnAPI.predict_median","page":"Predict and other operations","title":"LearnAPI.predict_median","text":"LearnAPI.predict_median(model, fitted_params, data...)\n\nSame as LearnAPI.predict except replaces probababilistic predictions with median values.\n\nNew model implementations\n\nA fallback broadcasts median over the first return value ŷ of LearnAPI.predict. An algorithm that computes probabilistic predictions may already need to predict mean values, and so overloading this method might enable a performance boost.\n\nIf overloaded, include :predict_median in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.predict, LearnAPI.fit.\n\n\n\n\n\n","category":"function"},{"location":"operations/#LearnAPI.predict_joint","page":"Predict and other operations","title":"LearnAPI.predict_joint","text":"LearnAPI.predict_joint(model, fitted_params, data...)\n\nFor a supervised learning model, return (d, report), where d is a single probability distribution for the sample space Y^n, where Y is the space in which the target variable associated with model takes its values. Here n is the number of observations in data.  Here fitted_params are the model's learned parameters, as returned by a preceding call to LearnAPI.fit. Here report contains ancilliary byproducts of the computation, or is nothing; data is a tuple of data objects, generally a single object representing new observations not seen in training. .\n\nWhile the interpretation of this distribution depends on the model, marginalizing component-wise will generally deliver correlated univariate distributions, and these will generally not agree with those returned by LearnAPI.predict, if also implemented.\n\nNew model implementations\n\nOnly implement this method if model has an associated concept of target variable, as defined in the LearnAPI.jl documentation. A trait declaration LearnAPI.target_proxy_kind, such as\n\nLearnAPI.target_proxy_kind(::Type{SomeModel}) = (predict_joint=JointSampleable(),)\n\nis required. Here the possible kinds of target proxies are LearnAPI.Sampleable, LearnAPI.Distribution, and LearnAPI.LogDistribution.\n\nIf implemented, include :predict_joint in the tuple returned by the LearnAPI.implemented_methods trait. .\n\nSee also LearnAPI.fit, LearnAPI.predict.\n\n\n\n\n\n","category":"function"},{"location":"operations/#LearnAPI.transform","page":"Predict and other operations","title":"LearnAPI.transform","text":"LearnAPI.transform(model, fitted_params, data...)\n\nReturn (output, report), where output is some kind of transformation of data, provided by model, based on the learned parameters fitted_params, as returned by a preceding call to LearnAPI.fit(model, ...) (which could be nothing for models that do not generalize to new data, such as \"static transformers\"). Here report contains ancilliary byproducts of the computation, or is nothing; data is a tuple of data objects, generally a single object representing new observations not seen in training. \n\nNew model implementations\n\nIf implemented, include :transform in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.inverse_transform, LearnAPI.fit, LearnAPI.predict,\n\n\n\n\n\n","category":"function"},{"location":"operations/#LearnAPI.inverse_transform","page":"Predict and other operations","title":"LearnAPI.inverse_transform","text":"LearnAPI.inverse_transform(model, fitted_params, data)\n\nReturn (data_inverted, report), where data_inverted is valid input to the call\n\nLearnAPI.transform(model, fitted_params, data_inverted)\n\nHere report contains ancilliary byproducts of the computation, or is nothing; data is a tuple of data objects, generally a single object representing new observations not seen in training. \n\nTypically, the map\n\ndata -> first(inverse_transform(model, fitted_params, data))\n\nwill be an inverse, approximate inverse, right inverse, or approximate right inverse, for the map\n\ndata -> first(transform(model, fitted_params, data))\n\nFor example, if transform corresponds to a projection, inverse_transform is the corresponding embedding.\n\nNew model implementations\n\nIf implemented, include :transform in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.fit, LearnAPI.predict,\n\n\n\n\n\n","category":"function"},{"location":"accessor_functions/#Accessor-Functions","page":"Accessor Functions","title":"Accessor Functions","text":"","category":"section"},{"location":"accessor_functions/","page":"Accessor Functions","title":"Accessor Functions","text":"Summary. While byproducts of training are ordinarily recorded in the report component of the output of fit/update!/ingest!, some families of models report an itme that is likely shared by multiple model types, and it is useful to have common interface for accessing these directly. Training losses and feature importances are two examples.","category":"page"},{"location":"accessor_functions/","page":"Accessor Functions","title":"Accessor Functions","text":"LearnAPI.feature_importances\nLearnAPI.training_labels\nLearnAPI.training_losses\nLearnAPI.training_scores","category":"page"},{"location":"patterns/incremental_models/#Incremental-Models","page":"Incremental Models","title":"Incremental Models","text":"","category":"section"},{"location":"patterns/learning_a_probability_distribution/#Learning-a-Probability-Distribution","page":"Learning a Probability Distribution","title":"Learning a Probability Distribution","text":"","category":"section"},{"location":"patterns/dimension_reduction/#Dimension-Reduction","page":"Dimension Reduction","title":"Dimension Reduction","text":"","category":"section"},{"location":"patterns/time_series_forecasting/#Time-Series-Forecasting","page":"Time Series Forecasting","title":"Time Series Forecasting","text":"","category":"section"},{"location":"fit_update_and_ingest/#Fit,-update!-and-ingest!","page":"Fit, update and ingest","title":"Fit, update! and ingest!","text":"","category":"section"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"Summary. Models that learn, i.e., generalize to new data, must overload fit; the fallback performs no operation and returns all nothing. Implement update! if certain hyper-parameter changes do not necessitate retraining from scratch (e.g., increasing iteration parameters). Implement ingest! to implement incremental learning.","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"method fallback compulsory? requires\nLearnAPI.fit does nothing, returns (nothing, nothing, nothing) no \nLearnAPI.update! calls fit no LearnAPI.fit\nLearnAPI.ingest! none no LearnAPI.fit","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"All three methods above return a triple (fitted_params, state, report) whose components are explained under LearnAPI.fit below.  Items that might be returned in report include: feature rankings/importances, SVM support vectors, clustering centres, methods for visualizing training outcomes, methods for saving learned parameters in a custom format, degrees of freedom, deviances. Precisely what report includes might be controlled by model hyperparameters, especially if there is a performance cost to it's inclusion.","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"Implement fit unless all operations, such as predict and transform, ignore their fitted_params argument (which will be nothing). This is the case for many models that have hyperparameters, but do not generalize to new data, such as a basic DBSCAN clustering algorithm.","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"The update! method is intended for all subsequent calls to train a model using the same observations, but with possibly altered hyperparameters (model argument). A fallback implementation simply calls fit. The main use cases for implementing update are: (i) warm-restarting iterative models, and (ii) \"smart\" training of composite models, such as linear pipelines. Here \"smart\" means that hyperparameter changes only trigger the retraining of downstream components.","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"The ingest! method supports incremental learning (same hyperparameters, but new training observations). Like update!, it depends on the output a preceding fit or ingest! call.","category":"page"},{"location":"fit_update_and_ingest/","page":"Fit, update and ingest","title":"Fit, update and ingest","text":"LearnAPI.fit\nLearnAPI.update!\nLearnAPI.ingest!","category":"page"},{"location":"fit_update_and_ingest/#LearnAPI.fit","page":"Fit, update and ingest","title":"LearnAPI.fit","text":"LearnAPI.fit(model, verbosity, data...; metadata...)\n\nFit model to the provided data and metadata. With the exception of warnings, training should be silent if verbosity == 0. Lower values should suppress warnings; any integer ought to be admissible. Here:\n\nmodel is a property-accessible object whose properties are the hyper-parameters of some  machine learning algorithm; see also LearnAPI.ismodel.\ndata is a tuple of data objects with a common number of observations, for example, data = (X, y, w) where X is a table of features, y is a target vector with the same number of rows, and w a vector of per-observation weights.\nmetadata is for extra information pertaining to the data that is never iterated, for example, weights for target classes. Another example would be feature groupings in the group lasso algorithm.\n\nReturn value\n\nReturns a tuple (fitted_params, state, report) where:\n\nThe fitted_params is the model's learned parameters (eg, the coefficients in a linear model) in a form understood by model operations. An operation is a method, like LearnAPI.predict or LearnAPI.transform, that has signature (model, fitted_params, data....); do LearnAPI.OPERATIONS to list. If some training outcome of user-interest is not needed for operations, it should be part of report instead (see below).\nThe state is for passing to LearnAPI.update! or LearnAPI.ingest!. For models that implement neither, state should be nothing.\nThe report records byproducts of training not in the fitted_params.\n\nNew model implementations\n\nOverloading this method for new models is optional.  A fallback performs no computation, returning (nothing, nothing, nothing).\n\nNote that in LearnAPI.jl the word \"data\" is only defined informally, as an object generating \"observations\", which are not defined at all.\n\nnote: Note\nThe method is not permitted to mutate model. In particular, if model has a random number generator as a hyperparameter (property) then it must be copied before use.  \n\nIf implemented, include :fit in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.update!, LearnAPI.ingest!.\n\n\n\n\n\n","category":"function"},{"location":"fit_update_and_ingest/#LearnAPI.update!","page":"Fit, update and ingest","title":"LearnAPI.update!","text":"LearnAPI.update!(model, verbosity, fitted_params, state, data...; metadata...)d Based on the values of `state`, and `fitted_params` returned by a preceding call to\n\nLearnAPI.fit, LearnAPI.ingest!, or LearnAPI.update!, update a model's learned parameters, returning new (or mutated) state and fitted_params.\n\nIntended for retraining a model when the training data has not changed, but model properties (hyperparameters) may have changed. Specifically, the assumption is that data and metadata have the same values seen in the most recent call to fit/update!/ingest!.\n\nThe most common use case is for continuing the training of an iterative model: state is simply a copy of the model used in the last training call (fit, update! or ingest!) and this will include the current number of iterations as a property. If model and state differ only in the number of iterations (e.g., epochs in a neural network), which has increased, then the learned parameters (weights) are updated, rather computed ab initio. Otherwise, update! simply calls fit, to force retraining from scratch.\n\nIt is permitted to return mutated versions of state and fitted_params.\n\nFor incremental training (same model, new data) see instead LearnAPI.ingest!.\n\nReturn value\n\nSame as LearnAPI.fit, namely a tuple (fitted_params, state, report). See LearnAPI.fit for details.\n\nNew model implementations\n\nOverloading this method is optional. A fallback calls LearnAPIperforms.fit:\n\nLearnAPI.update!(model, verbosity, fitted_params, state, data...; metadata...) =\n    fit(model, verbosity, data; metadata...)\n\nNote that in LearnAPI.jl the word \"data\" is only defined informally, as an object generating \"observations\", which are not defined at all.\n\nnote: Note\nThe method is not permitted to mutate model. In particular, if model has a random number generator as a hyperparameter (property) then it must be copied before use.  \n\nIf implemented, include :fit in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.fit, LearnAPI.ingest!.\n\n\n\n\n\n","category":"function"},{"location":"fit_update_and_ingest/#LearnAPI.ingest!","page":"Fit, update and ingest","title":"LearnAPI.ingest!","text":"LearnAPI.ingest!(model, verbosity, fitted_params, state, data...; metadata...)\n\nFor a model that supports incremental learning, update the learned parameters using data, which has typically not been seen before.  The arguments state and fitted_params are the output of a preceding call to LearnAPI.fit, LearnAPI.ingest!, or LearnAPI.update!, of which mutated or new versions are returned.\n\nFor updating learned parameters using the same data but new hyperparameters, see instead LearnAPI.update!.\n\nFor incremental training, see instead `LearnAPI.ingest'.\n\nReturn value\n\nSame as LearnAPI.fit, namely a tuple (fitted_params, state, report). See LearnAPI.fit for details.\n\nNew model implementations\n\nImplementing this method is an optional. It has no fallback.\n\nnote: Note\nThe method is not permitted to mutate model. In particular, if model has a random number generator as a hyperparameter (property) then it must be copied before use.  \n\nIf implemented, include :fit in the tuple returned by the LearnAPI.implemented_methods trait. \n\nSee also LearnAPI.fit, LearnAPI.update!.\n\n\n\n\n\n","category":"function"},{"location":"","page":"Introduction","title":"Introduction","text":"<script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n<span style=\"color: #9558B2;font-size:4.5em;\">\nLearnAPI.jl</span>\n<br>\n<span style=\"color: #9558B2;font-size:1.6em;font-style:italic;\">\nA basic Julia interface for training and applying machine learning models </span>\n<br><br>","category":"page"},{"location":"#Quick-tours","page":"Introduction","title":"Quick tours","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"For developers wanting to IMPLEMENT the learn API for new ML models: See Anatomy of an Implementation.\nFor those who want to USE models implementing LearnAPI.jl: Basic fit/predict workflow is illustrated here.","category":"page"},{"location":"#Approach","page":"Introduction","title":"Approach","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Machine learning algorithms, also called models, have a complicated taxonomy. Grouping models, or modelling tasks, into a relatively small number of types, such as \"classifier\" and \"clusterer\", and attempting to impose uniform behaviour within each group, is challenging. In our experience developing the MLJ ecosystem, this either leads to limitations on the models that can be included in a general interface, or additional complexity needed to cope with exceptional cases. Even if a complete user interface for machine learning might benefit from such groupings, a basement-level API for ML should, in our view, avoid them.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"In LearnAPI model behaviour is articulated using a number of traits. There is no abstract type model hierarchy.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"That said, for certain models involving a \"target\" variable (understood in a rather general way - see below) there is a clear-cut classification based on the proxy for the target as actually output by the model. Probability distributions, confidence intervals and survival functions are examples of Target proxies. LearnAPI provides a trait for distinguishing such models based on the kind of target proxy.","category":"page"},{"location":"#Methods","page":"Introduction","title":"Methods","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"LearnAPI.jl is a base interface for machine learning algorithms in which behaviour is articulated using traits. It has no abstract model types, apart from an optional supertype Model. It provides the following methods, dispatched on model type:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"fit for regular training, overloaded if the model generalizes to new data, as in classical supervised learning\nupdate! for adding model iterations, or responding efficiently to other post-fitchanges in hyperparameters\ningest! for incremental learning\noperations, such as predict, transform and inverse_transform for applying the model to data not used for training\ncommon accessor functions, such as feature_importances and training_losses, for extracting, from training outcomes, information common to different types of models\nmodel traits, such as target_proxy_kind(model), for promising specific behaviour","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"There is flexibility about how much of the interface is implemented by a given model object model. A special trait implemented_methods(model) declares what has been explicitly implemented or overloaded to work with model.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Since this is a functional-style interface, fit returns model state, in addition to learned parameters, for passing to the optional update! and ingest! methods. These training methods also return a report component, for exposing byproducts of training different from learned parameters. Similarly, all operations also return a report component (important for models that do not generalize to new data).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Models can be supervised or not supervised, can generalize to new data observations, or not generalize. To ensure proper handling by client packages of probabilistic and other non-literal forms of target predictions (pdfs, confidence intervals, survival functions, etc) the kind of prediction can be flagged appropriately; see more at \"target\" below.","category":"page"},{"location":"#scope","page":"Introduction","title":"Scope and undefined notions","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The Learn API provides methods for training, applying, and saving machine learning models, and that is all. It does not specify an interface for data access or data resampling. However, LearnAPI.jl is predicated on a few basic undefined notions (in boldface) which some higher-level interface might decide to formalize:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"An object which generates ordered sequences of individual observations is called data. For example a DataFrame instance, from DataFrames.jl, is considered data, the observatons being the rows. A matrix can be considered data, but whether the observations are rows or columns is ambiguous and not fixed by LearnAPI.\nEach machine learning model's behaviour is governed by a number of user-specified hyperparameters. The regularization parameter in ridge regression is an example. Hyperparameters are data independent. For example, the number of target classes is not a hyperparameter.\nInformation needed for training that is not a model hyperparameter and not data is called metadata. Examples, include target class weights and group lasso feature groupings.\nSome models involve the notion of a target variable and generate output with the same form as the target, or, more generally, some kind of target proxy, such as probability distributions. A target proxy is something that can be paired with target data to obtain useful information about the model and the data that has been presented to it, typically a measure of the model's expected performance on unseen data. A target variable is not necessarily encountered during training, i.e., target variables can make sense for unsupervised models, and also for models that do not generalize to new observations.  For examples, and an informal classification of target proxy types, refer to Target proxies.","category":"page"},{"location":"#Contents","page":"Introduction","title":"Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Our opening observations notwithstanding, it is useful to have a guide to the interface, linked below, organized around common informally defined patterns or \"tasks\". However, the definitive specification of the interface is the Reference section.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Anatomy of an Implementation (Overview)\nCommon Implementation Patterns (User Guide)\nReference (Official Specification)\nTesting an Implementation","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Info\nIt is strongly recommended users read  Anatomy of an Implementation before consulting the guide or reference sections.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Note. In the future, LearnAPI.jl will become the new foundation for the MLJ toolbox created by the same developers. However, LearnAPI.jl is meant as a general purpose, standalone, lightweight API for machine learning algorithms (and has no reference to the \"machines\" used there).","category":"page"},{"location":"patterns/outlier_detection/#Outlier-Detection","page":"Outlier Detection","title":"Outlier Detection","text":"","category":"section"}]
}
